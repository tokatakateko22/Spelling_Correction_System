{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MaKTawypwSP5",
    "outputId": "e3752f09-9032-4271-9ec1-2053575e589b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets evaluate sacrebleu jiwer pandas torch\n",
    "\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    BartTokenizerFast,\n",
    "    BartForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6bw9PGo70BwT"
   },
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjWkYKYhwb1H"
   },
   "source": [
    "#Cell 2: Load Dataset Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xnyGHyvFwX8W",
    "outputId": "2f947e85-bc46-4246-fb7c-275deb5496b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 training sentences\n",
      "Loaded 1000 validation sentences\n",
      "Loaded 1000 test sentences\n"
     ]
    }
   ],
   "source": [
    "def load_sentences(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip().split('\\t')[0] for line in f if line.strip()]\n",
    "\n",
    "train_sentences = load_sentences('tune.tsv')[:5000]\n",
    "val_sentences = load_sentences('validation.tsv')[:1000]\n",
    "test_sentences = load_sentences('test.tsv')[:1000]\n",
    "\n",
    "print(f\"Loaded {len(train_sentences)} training sentences\")\n",
    "print(f\"Loaded {len(val_sentences)} validation sentences\")\n",
    "print(f\"Loaded {len(test_sentences)} test sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLW-QDF4wioC"
   },
   "source": [
    "#Cell 3: Error Generation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dYuYl50vwX--",
    "outputId": "c6821c15-a876-4d18-fdd7-e00407608039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10000 training pairs\n",
      "Generated 1000 validation pairs\n",
      "Generated 1000 test pairs\n"
     ]
    }
   ],
   "source": [
    "def introduce_errors(sentence, min_errors=3, max_errors=5):\n",
    "    words = sentence.split()\n",
    "    if not words:\n",
    "        return sentence\n",
    "\n",
    "    current_words = list(words)\n",
    "    num_errors = random.randint(min_errors, max_errors)\n",
    "\n",
    "    for _ in range(num_errors):\n",
    "        if not current_words:\n",
    "            break\n",
    "\n",
    "        word_idx = random.randint(0, len(current_words) - 1)\n",
    "        word = current_words[word_idx]\n",
    "        if len(word) < 1:\n",
    "            continue\n",
    "\n",
    "        op = random.choice([\"delete\", \"insert\", \"substitute\", \"transpose\", \"duplicate\"])\n",
    "\n",
    "        if op == \"delete\" and len(word) > 0:\n",
    "            pos = random.randint(0, len(word)-1)\n",
    "            word = word[:pos] + word[pos+1:]\n",
    "        elif op == \"insert\":\n",
    "            pos = random.randint(0, len(word))\n",
    "            word = word[:pos] + random.choice(string.ascii_lowercase) + word[pos:]\n",
    "        elif op == \"substitute\":\n",
    "            pos = random.randint(0, len(word)-1)\n",
    "            word = word[:pos] + random.choice(string.ascii_lowercase) + word[pos+1:]\n",
    "        elif op == \"transpose\" and len(word) > 1:\n",
    "            pos = random.randint(0, len(word)-2)\n",
    "            word = word[:pos] + word[pos+1] + word[pos] + word[pos+2:]\n",
    "        elif op == \"duplicate\":\n",
    "            current_words.insert(word_idx + 1, word)\n",
    "            continue\n",
    "\n",
    "        current_words[word_idx] = word\n",
    "\n",
    "    return ' '.join(current_words)\n",
    "\n",
    "def generate_pairs(sentences, versions=2):\n",
    "    pairs = []\n",
    "    for sent in sentences:\n",
    "        for _ in range(versions):\n",
    "            corrupted = introduce_errors(sent)\n",
    "            if corrupted != sent:\n",
    "                pairs.append({'input_text': corrupted, 'target_text': sent})\n",
    "    return pairs\n",
    "\n",
    "train_pairs = generate_pairs(train_sentences)\n",
    "val_pairs = generate_pairs(val_sentences, versions=1)\n",
    "test_pairs = generate_pairs(test_sentences, versions=1)\n",
    "\n",
    "print(f\"Generated {len(train_pairs)} training pairs\")\n",
    "print(f\"Generated {len(val_pairs)} validation pairs\")\n",
    "print(f\"Generated {len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y13pD8KSxb9O"
   },
   "source": [
    "#Cell 4: Create DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3n8E3GswYBh",
    "outputId": "0c17bd75-133f-4f4e-f1d5-9e7032eb8569"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "datasets = DatasetDict({\n",
    "    'train': Dataset.from_list(train_pairs),\n",
    "    'validation': Dataset.from_list(val_pairs),\n",
    "    'test': Dataset.from_list(test_pairs)\n",
    "})\n",
    "\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ljn_u2Nyzl8U"
   },
   "source": [
    "#Cell 5: Tokenization and Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603,
     "referenced_widgets": [
      "56708fbaddbd444f9a8a8d535a106c03",
      "afb5d1354fe1444481181267107eb067",
      "cc6a1523c7604c6cb7e8c9a68b38b4ea",
      "de6fa96cfdb441e48f2adf06b4fc834a",
      "7a994e2239cc4eaebee723e185bce89e",
      "308d2690199a496288d33e91fe7fac5a",
      "74bc3aaf70d647cfa04372c52e832beb",
      "64df08bad61f4487bfd83118d34e5cab",
      "f91bd2d7e2724e259e1f0e57dbdc0fdf",
      "b6b801f4ab4144009596bd4df48659f7",
      "3c13489328e74d90a6a863dae165cfa9",
      "f25b1db6c16d41c4a819fbcdc56b6ed5",
      "0557df321dc14f20afa371bcf11ea6b4",
      "6404d0b9edca48418056a6a91a3d16aa",
      "4d4d0e10d5b94d4aa6c00eabc1f3aa25",
      "e64098516c864da2bf68f92815ba1547",
      "0f0e08d346494e1cb2254ee5e228815e",
      "7f5629f06a5f419c8a553669d3daec4e",
      "0f6b1699a7684200b8adb1e109b5a591",
      "b55c06752ec545c48d688ae2176301ac",
      "0c3ac15eacf440699df3ede361e494fd",
      "6c1ed391251f4c7ea74249e44eacce5c",
      "b4faf27bd8a1460fa1432122082dd218",
      "6add7981e886474ab8cc29ca6aa6c3ce",
      "e5177815f8b54967a9cf026b592834e6",
      "f28bc3a9cf98459f981a1d77fc2cfb11",
      "d01ba9e91a1741c3b433249a0aaacf97",
      "e94ba745b0e24f0bac4986ce372d93ef",
      "977b8f847f70421db8f85a3986c9e09c",
      "bef5fd0765b84ef89d7dc877698c89d7",
      "bff684426f134ffd8cb321a6bf02f103",
      "3666ab89d9044908b6af731ed69e2761",
      "d337e46acd97464ebee5453759701c14",
      "6e18a3aaa8a8467f85e2a810d20e6835",
      "a1234f3cf2924d648dcd7dbaa827f334",
      "1136b4d8150e4a88b62ca2a4bcc222c1",
      "7703057453044d58a0c33f1787043afd",
      "41737bead75d4f7fa1f3b40b6b9c41bd",
      "a9cdf6fdb6804b199159ffbf03e3a20f",
      "7dd966a2fbf148a0a27171dce794ed07",
      "38dd3abf77004183bc75929965abb209",
      "89ce4df5264a448592f7dedafa0d9a23",
      "36294b4203934f72972fefc4609c375c",
      "f17a96e61377448c8677f670af022bed",
      "652fefa62f604937b3340642c9205fc1",
      "93c52728c2354beeb9888c18f89100c8",
      "5b61858f98764128bcb3e458362309cb",
      "f6039f562eff4916a23e738ea59b83b0",
      "00667fb4bdcb49fbb162ac02144068de",
      "dcc98cdb37454f4b839c85f776560785",
      "0f34110b941249c5bc7e657c9f5b8ef7",
      "c048d530e0a94d5aabeb9b71e6d8dcd6",
      "5239ce62e36a41fda6bb8a50c17ee79e",
      "18055505488d49b7b78b48e1ec68d6e2",
      "27a04f593f0443b08ad6bba7280b35c4",
      "46d5476e564143869fdf42bce88a95b9",
      "a1f5ed82d93c499d80bd5f49f1a632ec",
      "32363c3e45ee4e4caa53269f0498456f",
      "092c8c6b2a9f4d5ea704ff3603a37f51",
      "630eb41005274d2198fe49d7858f7244",
      "9858f6fff1044bca8ad89c509464ed11",
      "07ecd0aa186848dabd446a538762780b",
      "04b6c5afc73f4ab7b296d90e96867b3d",
      "f9dbc74b65e64dc99ea044017a005697",
      "149ff4fa4a1741ab89b26ab14ff9400e",
      "67194a9084734acfaef14039f870c0dd",
      "eb43a010fcb54046ac41582d77f416d2",
      "cead7550c6f3449988a40c8bf54b5f65",
      "7cc182f10e2944e893db0b4ff7731c92",
      "7d7134e453924604a1365b4533389f28",
      "825956de75f44e3a81dc37180bef9a32",
      "e224c024f9d34e21add22af8fcffd2f2",
      "f69c41f636954496a7189dea6e246168",
      "05113fd6ccfe4c4ab42fc8d61879f82e",
      "942a9ce374e34cb9a0a11e7e63c4a7dc",
      "f37ddc194d0749399321ef2991a44289",
      "f0a75500a1624d0aa3c2f122b3e8fb91"
     ]
    },
    "id": "Bm7qNYjowYEX",
    "outputId": "7bde9e2b-4283-4af6-ee8b-7bd88a7ae13b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56708fbaddbd444f9a8a8d535a106c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25b1db6c16d41c4a819fbcdc56b6ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4faf27bd8a1460fa1432122082dd218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e18a3aaa8a8467f85e2a810d20e6835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652fefa62f604937b3340642c9205fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d5476e564143869fdf42bce88a95b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb43a010fcb54046ac41582d77f416d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"input_text\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(text_target=examples[\"target_text\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = datasets.map(preprocess_function, batched=True)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHZpcimzzuwF"
   },
   "source": [
    "#Cell 6: Initialize Model and Training Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "b6d9f74b31474bacbcdbf4ee9363e4a5",
      "eb00238c84f841799975b89baabb0a34",
      "e57ce79a0f444bb78499e8ae6f4371aa",
      "61ef0b6f8aaf4e1d8b5837da47a2ccd2",
      "0aa9e7d02bfb40bd9c2a290d76d5d889",
      "d102c8daf84443e4bec83e160b38fc87",
      "f2a77ffa0316455fb7706f89d5ac766b",
      "7594ef9b58394ebe9c419dc81ef81ce2",
      "98eff297f6f5417295ac5c12f0ed4cb5",
      "05bdadeed793456f9394e28b5d611f93",
      "b7addf039ebb4ecf9791078c8477b2d5",
      "9cb4a678f59649f4affe68dbad529ba1",
      "0cea154544b841649d1a199b986103bc",
      "990efd05247a4a6c8e2ba680da0b38c7",
      "c0051fad64f8455dbf77674f984bf7d2",
      "597b47589e6f4af7bed2dc6ec51076c0",
      "12048b5ccb1e4a8ea9288b3648d822e1",
      "5b46ae0fd82d42b9bbf5d354b1d9b904",
      "fc295bbcd0d740a48472ed678a7f6b2c",
      "33318f1b76c3497fa5dc3d5c6895cf11",
      "c066f08c208542b2b8996c3dc80f216f",
      "b5e1443cb3ab4350addf14a8ab3ac222",
      "ce09c18c2003470698a432e921b00217",
      "e2efe530753f45de9c929056fbeda04f",
      "c413d6427ba24838a097a0ad9e291dd5",
      "4cf655b4fa604773b438293bd83d3b6e",
      "0827fb193f9241efbea3cc840e8b5423",
      "a88621e069b840c88b00d975b191ae11",
      "3252595b798543f79790d4fd48db0af5",
      "9885ffcd3d32406f970514c69c04f660",
      "49a33a5e47b84b1aae314f869896dd83",
      "d67638bef9a9448f8c0bcf46cecc5a23",
      "a55778cd8ba54500b331b9fcfb0e6c36"
     ]
    },
    "id": "l8II5-YEwYHJ",
    "outputId": "6ec9062f-5c8b-4417-d0cf-ae87077b54ad"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d9f74b31474bacbcdbf4ee9363e4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb4a678f59649f4affe68dbad529ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce09c18c2003470698a432e921b00217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu = bleu_metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n",
    "\n",
    "    return {\"wer\": wer, \"bleu\": bleu[\"score\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5d-5yZI1Id7"
   },
   "source": [
    "#Cell 7: Training Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SwoBPS5AwYJq",
    "outputId": "9e65a1ed-0b2b-44fd-a6dc-0c477faf0a55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-e575e2e1ac8a>:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"spelling_correction_model\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BORFHSci1OFV"
   },
   "source": [
    "#Cell 8: Training and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "8sW2AZ0Y1DRu",
    "outputId": "e75029bf-1397-4ae6-b7d9-966b74372545"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 09:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.604800</td>\n",
       "      <td>0.049643</td>\n",
       "      <td>0.561572</td>\n",
       "      <td>27.841936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.043540</td>\n",
       "      <td>0.558474</td>\n",
       "      <td>28.244433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.043014</td>\n",
       "      <td>0.557825</td>\n",
       "      <td>28.379416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "WER: 0.5567\n",
      "BLEU: 28.6030\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "test_results = trainer.evaluate(\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    metric_key_prefix=\"test\"\n",
    ")\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"WER: {test_results['test_wer']:.4f}\")\n",
    "print(f\"BLEU: {test_results['test_bleu']:.4f}\")\n",
    "\n",
    "trainer.save_model(\"final_spelling_correction_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcqX_fqU1T18"
   },
   "source": [
    "#Cell 9: Example Corrections\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lFwlZhBW1DUH",
    "outputId": "18f02644-2c4d-4e68-984f-cf8c29f5090b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying predictions using `trainer.model`...\n",
      "\n",
      "--- Example Predictions on Test Set (first 10 examples) ---\n",
      "------------------------------------------------------------\n",
      "Example 1:\n",
      "  Input (Misspelled)   : \"' Bandolier  Budgie ' , ia free iTunes app for iPad , iPhone and iPod touch , released in December 2011 , tells the story of the making of Bandolier in the band 's own words - including including an extensive audio interview with Burke Shelley \"\n",
      "  Predicted (Corrected): \"' Bandolier , Budgie ' , a free iTunes app for iPad , iPhone and iPod touch , released in December 2011 , tells the story of the making of Bandoliers in the band 's own words - including an extensive audio interview with Burke Shelley .\"\n",
      "  Actual (Correct)     : \"' Bandolier - Budgie ' , a free iTunes app for iPad , iPhone and iPod touch , released in December 2011 , tells the story of the making of Bandolier in the band 's own words - including an extensive audio interview with Burke Shelley .\"\n",
      "------------------------------------------------------------\n",
      "Example 2:\n",
      "  Input (Misspelled)   : \"' Eden Black ' was grown from seed in the late 1980s by Stephen Stephen Morley e under his conditions it produces pitchers that aer almost completley black .\"\n",
      "  Predicted (Corrected): \"' Eden Black ' was grown from seed in the late 1980s by Stephen Morley ; under his conditions it produces pitchers that are almost black .\"\n",
      "  Actual (Correct)     : \"' Eden Black ' was grown from seed in the late 1980s by Stephen Morley , under his conditions it produces pitchers that are almost completley black .\"\n",
      "------------------------------------------------------------\n",
      "Example 3:\n",
      "  Input (Misspelled)   : \"' Wilson should extend hsi tint on The Voice to renew public interest in the band ; given that they 're pulling out all the stops , they deseerve all the acclaim that surrounded them for their first two albums .\"\n",
      "  Predicted (Corrected): \"' Wilson should extend his stint on The Voice to renew public interest in the band ; given that they 're pulling out all the stops , they deserve all the acclaim that surrounded them for their first two albums .\"\n",
      "  Actual (Correct)     : \"' Wilson should extend his stint on The Voice to renew public interest in the band ; given that they 're pulling out all the stops , they deserve all the acclaim that surrounded them for their first two albums .\"\n",
      "------------------------------------------------------------\n",
      "Example 4:\n",
      "  Input (Misspelled)   : \"'' '' New York Mining Disaster 1941 '' '' was the second EP released by the Bee Gees in 1967 an the Spin Records , like their first kq , it way released only in Australia .\"\n",
      "  Predicted (Corrected): \"'' New York Mining Disaster 1941 '' was the second EP released by the Bee Gees in 1967 on the Spin Records , like their first , it was released only in Australia .\"\n",
      "  Actual (Correct)     : \"'' '' New York Mining Disaster 1941 '' '' was the second EP released by the Bee Gees in 1967 on the Spin Records , like their first EP , it was released only in Australia .\"\n",
      "------------------------------------------------------------\n",
      "Example 5:\n",
      "  Input (Misspelled)   : \"'' ADAPTOGENS : Herdbs for Strength , , Stamina , and Stress Relief , '' Healing Arts Press , 2007 - contains a detailed monograph on Schisandra xhinensis as welg as highlights health benefits .\"\n",
      "  Predicted (Corrected): \"'' ADAPTOGENS : Herbs for Strength , Stamina , and Stress Relief , '' Healing Arts Press , 2007 - contains a detailed monograph on Schisandra xhinensis as well as highlights health benefits .\"\n",
      "  Actual (Correct)     : \"'' ADAPTOGENS : Herbs for Strength , Stamina , and Stress Relief , '' Healing Arts Press , 2007 - contains a detailed monograph on Schisandra chinensis as well as highlights health benefits .\"\n",
      "------------------------------------------------------------\n",
      "Example 6:\n",
      "  Input (Misspelled)   : \"'' Aerodynamic '' is an instrumental song qy Daft Punk tht is particularly well - - known for its robotic guitar solo .\"\n",
      "  Predicted (Corrected): \"'' Aerodynamic '' is an instrumental song by Daft Punk that is particularly well - known for its robotic guitar solo .\"\n",
      "  Actual (Correct)     : \"'' Aerodynamic '' is an instrumental song by Daft Punk that is particularly well - known for its robotic guitar solo .\"\n",
      "------------------------------------------------------------\n",
      "Example 7:\n",
      "  Input (Misspelled)   : \"g'' After a lawsuit by Ray Lynch for allegedly not paying him ,l the company was foreclosed by Security Pacific Bank on November 1991 with all of its assets osld by June 1992 .\"\n",
      "  Predicted (Corrected): \"'' After a lawsuit by Ray Lynch for allegedly not paying him , the company was foreclosed by Security Pacific Bank on November 1991 with all of its assets sold by June 1992 .\"\n",
      "  Actual (Correct)     : \"'' After a lawsuit by Ray Lynch for allegedly not paying him , the company was foreclosed by Security Pacific Bank on November 1991 with all of its assets sold by June 1992 .\"\n",
      "------------------------------------------------------------\n",
      "Example 8:\n",
      "  Input (Misspelled)   : \"'' Again Schwartz Schwartz lcaims 100 % hits for Susy Smith , buct nothing ofr his true grandmothers r\"\n",
      "  Predicted (Corrected): \"'' Again Schwartz hits for Susy Smith , but nothing of his true grandmothers .\"\n",
      "  Actual (Correct)     : \"'' Again Schwartz claims 100 % hits for Susy Smith , but nothing for his true grandmothers .\"\n",
      "------------------------------------------------------------\n",
      "Example 9:\n",
      "  Input (Misspelled)   : \"'n' Bellringer '' was in factj a derivative of '' Hellbringer , '' a nickname given to him by flelow musician Dan Massie in reference to hs unquenchable thirsit for debauchery and outlandish clothing .\"\n",
      "  Predicted (Corrected): \"'' Bellringer '' was in fact a derivative of '' Hellbringer , '' a nickname given to him by legendary musician Dan Massie in reference to his unquenchable thirst for debauchery and outlandish clothing .\"\n",
      "  Actual (Correct)     : \"'' Bellringer '' was in fact a derivative of '' Hellbringer , '' a nickname given to him by fellow musician Dan Massie in reference to his unquenchable thirst for debauchery and outlandish clothing .\"\n",
      "------------------------------------------------------------\n",
      "Example 10:\n",
      "  Input (Misspelled)   : \"'' Chalayil mahavishnu kshetarm '' '' is a famous Vishnu temple situated in Mattanur ii one of the rare temples for worsshiping Narasimha avathara of lord Vishnu .\"\n",
      "  Predicted (Corrected): \"'' Chalayil mahavishnu kshetarm '' is a famous Vishnu temple situated in Mattanur , one of the rare temples for worshipping Narasimha avathara of lord Vishnu .\"\n",
      "  Actual (Correct)     : \"'' Chalayil mahavishnu kshetram '' is a famous Vishnu temple situated in Mattanur is one of the rare temples for worshiping Narasimha avathara of lord Vishnu .\"\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Displaying predictions using `trainer.model`...\\n\")\n",
    "print(\"--- Example Predictions on Test Set (first 10 examples) ---\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "\n",
    "for i in range(10):\n",
    "    example = datasets[\"test\"][i]\n",
    "    inputs = tokenizer(example[\"input_text\"], return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_length=128)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Input (Misspelled)   : {repr(example['input_text'])}\")\n",
    "    print(f\"  Predicted (Corrected): {repr(prediction)}\")\n",
    "    print(f\"  Actual (Correct)     : {repr(example['target_text'])}\")\n",
    "    print(\"------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
