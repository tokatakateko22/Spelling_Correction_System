{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "MaKTawypwSP5",
    "outputId": "e3752f09-9032-4271-9ec1-2053575e589b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets evaluate sacrebleu jiwer pandas torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6bw9PGo70BwT"
   },
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    T5TokenizerFast,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjWkYKYhwb1H"
   },
   "source": [
    "#Cell 2: Load Dataset Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xnyGHyvFwX8W",
    "outputId": "8d738d84-1b58-45ff-cf50-01c615c0f5e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 training sentences\n",
      "Loaded 5000 validation sentences\n",
      "Loaded 5000 test sentences\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Dataset Files (using dictionaries)\n",
    "def load_sentences(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None, usecols=[0])\n",
    "    return {i: sent for i, sent in enumerate(df[0].tolist())}\n",
    "\n",
    "train_sentences = load_sentences('tune.tsv')\n",
    "val_sentences = load_sentences('validation.tsv')\n",
    "test_sentences = load_sentences('test.tsv')\n",
    "\n",
    "print(f\"Loaded {len(train_sentences)} training sentences\")\n",
    "print(f\"Loaded {len(val_sentences)} validation sentences\")\n",
    "print(f\"Loaded {len(test_sentences)} test sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLW-QDF4wioC"
   },
   "source": [
    "#Cell 3: Error Generation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dYuYl50vwX--",
    "outputId": "6dfd1989-3109-4891-ac04-65f2f212b5d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10000 training pairs\n",
      "Generated 5000 validation pairs\n",
      "Generated 5000 test pairs\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Error Generation Functions\n",
    "def introduce_errors(sentence, min_errors=3, max_errors=5):\n",
    "    words = sentence.split()\n",
    "    if not words:\n",
    "        return sentence\n",
    "\n",
    "    current_words = list(words)\n",
    "    num_errors = random.randint(min_errors, max_errors)\n",
    "\n",
    "    for _ in range(num_errors):\n",
    "        if not current_words:\n",
    "            break\n",
    "\n",
    "        word_idx = random.randint(0, len(current_words) - 1)\n",
    "        word = current_words[word_idx]\n",
    "        if len(word) < 1:\n",
    "            continue\n",
    "\n",
    "        op = random.choice([\"delete\", \"insert\", \"substitute\", \"transpose\", \"duplicate\"])\n",
    "\n",
    "        if op == \"delete\" and len(word) > 0:\n",
    "            pos = random.randint(0, len(word)-1)\n",
    "            word = word[:pos] + word[pos+1:]\n",
    "        elif op == \"insert\":\n",
    "            pos = random.randint(0, len(word))\n",
    "            word = word[:pos] + random.choice(string.ascii_lowercase) + word[pos:]\n",
    "        elif op == \"substitute\":\n",
    "            pos = random.randint(0, len(word)-1)\n",
    "            word = word[:pos] + random.choice(string.ascii_lowercase) + word[pos+1:]\n",
    "        elif op == \"transpose\" and len(word) > 1:\n",
    "            pos = random.randint(0, len(word)-2)\n",
    "            word = word[:pos] + word[pos+1] + word[pos] + word[pos+2:]\n",
    "        elif op == \"duplicate\":\n",
    "            current_words.insert(word_idx + 1, word)\n",
    "            continue\n",
    "\n",
    "        current_words[word_idx] = word\n",
    "\n",
    "    return ' '.join(current_words)\n",
    "\n",
    "def generate_pairs(sentences_dict, versions=2):\n",
    "    pairs = []\n",
    "    for sent_id, sent in sentences_dict.items():\n",
    "        for _ in range(versions):\n",
    "            corrupted = introduce_errors(sent)\n",
    "            if corrupted != sent:\n",
    "                pairs.append({\n",
    "                    'id': f\"{sent_id}_{_}\",\n",
    "                    'input_text': corrupted,\n",
    "                    'target_text': sent\n",
    "                })\n",
    "    return pairs\n",
    "\n",
    "train_pairs = generate_pairs(train_sentences)\n",
    "val_pairs = generate_pairs(val_sentences, versions=1)\n",
    "test_pairs = generate_pairs(test_sentences, versions=1)\n",
    "\n",
    "print(f\"Generated {len(train_pairs)} training pairs\")\n",
    "print(f\"Generated {len(val_pairs)} validation pairs\")\n",
    "print(f\"Generated {len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y13pD8KSxb9O"
   },
   "source": [
    "#Cell 3.1: Create DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3n8E3GswYBh",
    "outputId": "33104413-928c-403a-8d49-cd1cd492cc9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'input_text', 'target_text'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'input_text', 'target_text'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'input_text', 'target_text'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "datasets = DatasetDict({\n",
    "    'train': Dataset.from_list(train_pairs),\n",
    "    'validation': Dataset.from_list(val_pairs),\n",
    "    'test': Dataset.from_list(test_pairs)\n",
    "})\n",
    "\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ljn_u2Nyzl8U"
   },
   "source": [
    "#Cell 4: Tokenization and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385,
     "referenced_widgets": [
      "eccc822881014916aa8441bf7672e58b",
      "0cfa3cb4e2324617b6ce6075e82b6a9f",
      "702e82e1cdf14c38bad732942adb3921",
      "6047b015ed29449f8f150db0a10ad4bc",
      "05bd7f653b8b4237b2a8cc510255fda4",
      "21041166457b497281f5463e5bb0f822",
      "210f86b13b8a4a11b014b502d5148f61",
      "09a0408c1e08473484ee15ed316b09e4",
      "93c91940f5e445689d6b0a10a3831876",
      "66ce8cc8b5ff435ba085f16808f4ecb9",
      "c6578d2bb7344b24832e9be14bc79ba3",
      "850cdfd6e54c401c83a7ed5e82778b38",
      "974bb64a2d874878a4854ecf97f50b95",
      "28f11b6a6c4d4d769c9e645e455014db",
      "2b92d47e3a1741dab31779f61cb452b2",
      "b70230a24f0147c48b09877c728c0ce6",
      "e553f21f6f194efe8f1bf488299bd0d2",
      "9cfea33962d84b1dae900f77872701cd",
      "bbace393ca794951b4800eb2d2c05e7e",
      "2ec70bd60bb84aeaacbfb57b3dd88b6c",
      "9baefd08d3574693a8df543dc3885783",
      "9118a528184a4ff38f11c804aa6d7054",
      "ded567a7370849b7b8145854845c38b8",
      "bcb3438e3d0e4b018b61a703d072b186",
      "85828a86deb942d2abb4d79b83e8fe15",
      "22aef43dac3f4acd842ef4044aa83d47",
      "51d9d25c8e7c4d039ea616069e1a74b7",
      "f11a50079f1c48f5ac73fa3acc6ee8da",
      "4dae829e413d4d549bb684f12bcd2cd6",
      "82236055405e466693cfe1d974727373",
      "b26bb18928a04ffbbccc89c3e778ee96",
      "0d2f746df09949039a0c95bdc68b235f",
      "d9bf630788104f68bbea0aec07c5453b"
     ]
    },
    "id": "Bm7qNYjowYEX",
    "outputId": "2ab6abbe-ceef-48c3-e163-ea6850ff1f93"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccc822881014916aa8441bf7672e58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850cdfd6e54c401c83a7ed5e82778b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded567a7370849b7b8145854845c38b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Tokenization and Data Preparation\n",
    "MODEL_NAME = \"t5-small\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize input and target sequences\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\" # Added padding here\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"target_text\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\" # Added padding here\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization using Dataset.map()\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# The data collator will handle batching and converting to tensors\n",
    "# No need to manually create TensorDatasets\n",
    "\n",
    "print(\"Tokenized dataset structure:\")\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHZpcimzzuwF"
   },
   "source": [
    "#Cell 5: Initialize Model and Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "l8II5-YEwYHJ"
   },
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu = bleu_metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n",
    "\n",
    "    return {\"wer\": wer, \"bleu\": bleu[\"score\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5d-5yZI1Id7"
   },
   "source": [
    "#Cell 6: Training Configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SwoBPS5AwYJq",
    "outputId": "7a3f6a08-89ed-4189-d3cd-95658b15da36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-7564c5169b6a>:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Training Configuration\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"t5_spelling_correction\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    # Use the tokenized datasets instead of the original TensorDatasets\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BORFHSci1OFV"
   },
   "source": [
    "#Cell 7: Training and Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "8sW2AZ0Y1DRu",
    "outputId": "b57d910e-90ed-4360-f47e-e4799f47667a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 31:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.047700</td>\n",
       "      <td>0.122922</td>\n",
       "      <td>0.633578</td>\n",
       "      <td>17.600643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.140300</td>\n",
       "      <td>0.103471</td>\n",
       "      <td>0.610293</td>\n",
       "      <td>20.327078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.124000</td>\n",
       "      <td>0.096333</td>\n",
       "      <td>0.603981</td>\n",
       "      <td>21.089498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.115500</td>\n",
       "      <td>0.092220</td>\n",
       "      <td>0.600580</td>\n",
       "      <td>21.498507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.110800</td>\n",
       "      <td>0.090253</td>\n",
       "      <td>0.599753</td>\n",
       "      <td>21.584418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.088748</td>\n",
       "      <td>0.598886</td>\n",
       "      <td>21.698682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.087849</td>\n",
       "      <td>0.598138</td>\n",
       "      <td>21.779551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 03:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "WER: 0.5999\n",
      "BLEU: 21.7090\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Training and Evaluation\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "test_results = trainer.evaluate(\n",
    "    # Use the tokenized test dataset\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    metric_key_prefix=\"test\"\n",
    ")\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"WER: {test_results['test_wer']:.4f}\")\n",
    "print(f\"BLEU: {test_results['test_bleu']:.4f}\")\n",
    "\n",
    "trainer.save_model(\"t5_small_spelling_correction\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcqX_fqU1T18"
   },
   "source": [
    "#Cell 9: Example Corrections\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lFwlZhBW1DUH",
    "outputId": "038550ad-ab42-418e-80d4-8782b939f93c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying predictions using `trainer.model`...\n",
      "\n",
      "--- Example Predictions on Test Set (first 10 examples) ---\n",
      "------------------------------------------------------------\n",
      "Example 1:\n",
      "  Input (Misspelled)   : \"' Bandolier - Budgie ' , a free iTunes app for iPad , iPhone and iPod touch , released n December 2011 , tells the story of the making of Bandolier in the band s' own words r- including an extensive audio interview with Burke Shelley .\"\n",
      "  Predicted (Corrected): \"' Bandolier - Budgie ', a free iTunes app for iPad, iPhone and iPod touch, released in December 2011, tells the story of the making of Bandolier in the band s own words - including an extensive audio interview with Burke Shelley.\"\n",
      "  Actual (Correct)     : \"' Bandolier - Budgie ' , a free iTunes app for iPad , iPhone and iPod touch , released in December 2011 , tells the story of the making of Bandolier in the band 's own words - including an extensive audio interview with Burke Shelley .\"\n",
      "------------------------------------------------------------\n",
      "Example 2:\n",
      "  Input (Misspelled)   : \"' Eden Black ' was grown from seed in the late 1980s by Stephen Morley , under his conditions it produces pitchers that rae are almots completley black .\"\n",
      "  Predicted (Corrected): \"' Eden Black'was grown from seed in the late 1980s by Stephen Morley, under his conditions it produces pitchers that are almots completley black.\"\n",
      "  Actual (Correct)     : \"' Eden Black ' was grown from seed in the late 1980s by Stephen Morley , under his conditions it produces pitchers that are almost completley black .\"\n",
      "------------------------------------------------------------\n",
      "Example 3:\n",
      "  Input (Misspelled)   : \"' Wilson should extend his stint on The Voice to renew public interest in the band ; given that they 're pulling oit all the stops , they deserve all the acclaim that surrounded surrounded surrounded surrounded surrounded them for their first two albums .\"\n",
      "  Predicted (Corrected): \"Wilson should extend his stint on The Voice to renew public interest in the band ; given that they're pulling out all the stops, they deserve all the acclaim that surrounded them for their first two albums.\"\n",
      "  Actual (Correct)     : \"' Wilson should extend his stint on The Voice to renew public interest in the band ; given that they 're pulling out all the stops , they deserve all the acclaim that surrounded them for their first two albums .\"\n",
      "------------------------------------------------------------\n",
      "Example 4:\n",
      "  Input (Misspelled)   : \"'' '' New York Mining Disaster Disaster 1941 '' '' was the second EP released by the Bee Gees in 1967 on the Spin Records , like their first EP , ii was released only in Australia .h\"\n",
      "  Predicted (Corrected): \"'' New York Mining Disaster 1941 '' was the second EP released by the Bee Gees in 1967 on the Spin Records, like their first EP, i was released only in Australia.\"\n",
      "  Actual (Correct)     : \"'' '' New York Mining Disaster 1941 '' '' was the second EP released by the Bee Gees in 1967 on the Spin Records , like their first EP , it was released only in Australia .\"\n",
      "------------------------------------------------------------\n",
      "Example 5:\n",
      "  Input (Misspelled)   : \"'' ADAPTOGENS : Herbs for Strength , Stamina , and Stress Relief , '' Healing Arts Press b, 2007 2007 - contains contains a detailed monograph on Schisanra chinensis as well as highlights hejlth benefits .\"\n",
      "  Predicted (Corrected): \"'' ADAPTOGENS : Herbs for Strength, Stamina, and Stress Relief, '' Healing Arts Press, 2007 - contains a detailed monograph on Schisanra chinensis as well as highlights the benefits.\"\n",
      "  Actual (Correct)     : \"'' ADAPTOGENS : Herbs for Strength , Stamina , and Stress Relief , '' Healing Arts Press , 2007 - contains a detailed monograph on Schisandra chinensis as well as highlights health benefits .\"\n",
      "------------------------------------------------------------\n",
      "Example 6:\n",
      "  Input (Misspelled)   : \"'' Aerodynamhc Aerodynamic '' is an instrumental song by Daft Punk tht is particularly well - known for its robotic guitar solo .\"\n",
      "  Predicted (Corrected): \"'' Aerodynamhc Aerodynamic '' is an instrumental song by Daft Punk and is particularly well - known for its robotic guitar solo.\"\n",
      "  Actual (Correct)     : \"'' Aerodynamic '' is an instrumental song by Daft Punk that is particularly well - known for its robotic guitar solo .\"\n",
      "------------------------------------------------------------\n",
      "Example 7:\n",
      "  Input (Misspelled)   : \"' After a lawsuit by Ray Lynch for allegedly not paying him , the company was foreclosed by Security Pacific Bank on November 1991 1991 with atll of its assets sold by June 1992 .\"\n",
      "  Predicted (Corrected): \"' After a lawsuit by Ray Lynch for allegedly not paying him, the company was foreclosed by Security Pacific Bank on November 1991 with the total of its assets sold by June 1992.\"\n",
      "  Actual (Correct)     : \"'' After a lawsuit by Ray Lynch for allegedly not paying him , the company was foreclosed by Security Pacific Bank on November 1991 with all of its assets sold by June 1992 .\"\n",
      "------------------------------------------------------------\n",
      "Example 8:\n",
      "  Input (Misspelled)   : \"'' Agavin Schartz claims 100 % hits for Susy Smith , but nothing for his true grandmothers grandmothers n\"\n",
      "  Predicted (Corrected): \"'' Agavin Schartz claims 100 % hits for Susy Smith, but nothing for his true grandmothers.\"\n",
      "  Actual (Correct)     : \"'' Again Schwartz claims 100 % hits for Susy Smith , but nothing for his true grandmothers .\"\n",
      "------------------------------------------------------------\n",
      "Example 9:\n",
      "  Input (Misspelled)   : \"'' Bellringer '' was in fact a derivative of '' Hellbringer , '' a nickname given to him by fellow musician Dan Massie i reference yto his unquenchable thirst for debauchery nad outlandish clothing .\"\n",
      "  Predicted (Corrected): \"'' Bellringer '' was in fact a derivative of '' Hellbringer, '' a nickname given to him by fellow musician Dan Massie, referring to his unquenchable thirst for debauchery and outlandish clothing.\"\n",
      "  Actual (Correct)     : \"'' Bellringer '' was in fact a derivative of '' Hellbringer , '' a nickname given to him by fellow musician Dan Massie in reference to his unquenchable thirst for debauchery and outlandish clothing .\"\n",
      "------------------------------------------------------------\n",
      "Example 10:\n",
      "  Input (Misspelled)   : \"'' Chalayil mahavishnu kshetram '' is a famous Vishnu temple stiuated hn Mattanur is one of the rare templers for worshiping Narasimha avathara of lord Vishnu .\"\n",
      "  Predicted (Corrected): \"'' Chalayil mahavishnu kshetram '' is a famous Vishnu temple stiuated by Mattanur is one of the rare templers for worshiping Narasimha avathara of the Lord Vishnu.\"\n",
      "  Actual (Correct)     : \"'' Chalayil mahavishnu kshetram '' is a famous Vishnu temple situated in Mattanur is one of the rare temples for worshiping Narasimha avathara of lord Vishnu .\"\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Displaying predictions using `trainer.model`...\\n\")\n",
    "print(\"--- Example Predictions on Test Set (first 10 examples) ---\")\n",
    "print(\"------------------------------------------------------------\")\n",
    "\n",
    "for i in range(10):\n",
    "    example = datasets[\"test\"][i]\n",
    "    inputs = tokenizer(example[\"input_text\"], return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_length=128)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Input (Misspelled)   : {repr(example['input_text'])}\")\n",
    "    print(f\"  Predicted (Corrected): {repr(prediction)}\")\n",
    "    print(f\"  Actual (Correct)     : {repr(example['target_text'])}\")\n",
    "    print(\"------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
